### Currently working on:

| Idea                                               | Status                                                                                     | References/Papers                                                 | 
|----------------------------------------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------|
| FIFO CogVideoX                                     | In Progress                                                                                |   https://jjihwan.github.io/projects/FIFO-Diffusion |
| Flux Image Inversion using RNRI                    | Blocked. Understanding of prior for flow matching too low                                  |   https://barakmam.github.io/rnri.github.io/| 
| CogVideoX Attention Scaling                        | Paused. Need to recheck for higher res                                                     |   https://arxiv.org/abs/2306.08645| 
| RB Modulation for FLUX                             | Planned. SOC is straightforward. AFA needs to be seen in detail. Planning to replace CSD with other image opeerators for different manifold explorations. Soft histograms for relative color palette retention ?                            |   https://rb-modulation.github.io/| 
| CogVideoX distillation using FineVideo and PeRFlow | Planned. Needs compute grant. May be scrapped once BFLs video model is out.                |   https://arxiv.org/abs/2405.07510| 
| Underwater Image Colorization as an Inverse Problem| Planned. Needs better underestanding of inverse problems                                   |   https://github.com/LituRout/PSLD| 
| Flux generation steering using SAE for CLIP        | Planned. Need better understanding of SAEs & apply them to T5 as well |   https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/interpreting-and-steering-features-in-images| 
| LoRA ControlNet layer        | Planned. Compute âˆ†W for Flux dev & its controlnet layer. Decompose to LoRA and see decomposition error. If its low enough, LoRA should be enough |   [ChatGPT conversation](https://chatgpt.com/share/66f12970-6608-800f-a24e-20c4d2766c4a)| 
| Transformer layers as Painters for DiTs            | Complete. Results published [here](https://huggingface.co/blog/NagaSaiAbhinay/transformer-layers-as-painters-dit)|   https://arxiv.org/abs/2407.09298| 
